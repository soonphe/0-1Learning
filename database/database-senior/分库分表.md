# 0-1Learning

![alt text](../../static/common/svg/luoxiaosheng.svg "公众号")
![alt text](../../static/common/svg/luoxiaosheng_learning.svg "学习")
![alt text](../../static/common/svg/luoxiaosheng_wechat.svg "微信")


## 分库分表常见问题和解决方案

### MySQL大数据表出现的性能问题
- 表数据量过大
- sql查询太复杂
- sql查询没走索引
- 数据库服务器的性能过低等

### 大数据表优化方案
对于大数据表的优化最直观的方式就是减少单表数据量，所以常见的解决方案是:

- 分库分表，大表拆小表。
- 冷热数据分离，所谓的冷热数据，其实就是根据访问频次来划分的，访问频次较多的数据是热数据，访问频次少的数据是冷数据。冷热数据分离就是把这两类数据分离到不同的表中，从而减少热数据表的大小。
- 历史数据归档，简单来说就是把时间比较久远的数据分离出来存档，保证实时库的数据的有效生命周期。(磁带等，低价的存储介质)

### 分表分库方案

分库分表是非常常见针对单个数据表数据量过大的优化方式，它的核心思想是把一个大的数据表拆分成多个小的数据表，这个过程也叫（数据分片），它的本质其实有点类似于传统数据库中的分区表，比如mysql和oracle都支持分区表机制。

分库分表是一种水平扩展手段，每个分片上包含原来总的数据集的一个子集。这种分而治之的思想在技术中很常见，比如多CPU、分布式架构、分布式缓存等等，像redis cluster集群，slot槽的分配就是一种数据分片的思想。

数据库分库分表一般有两种实现方式：
- 垂直拆分：
    - 单库垂直分表：
      - 单个表的字段数量建议控制在20~50个之间，之所以建议做这个限制，是因为如果字段加上数据累计的长度超过一个阈值后，数据就不是存储在一个页上，就会产生分页的问题，而这个问题会导致查询性能下降。
      - 所以如果当某些业务表的字段过多时，我们一般会拆去垂直拆分的方式，把一个表的字段拆分成多个表，如：把一个订单表垂直拆分成一个订单主表和一个订单明细表。
      - 在Innodb引擎中，单表字段最大限制为1017，参考mysql官网。
    - 多库垂直分表：
      - 多库垂直拆分实际上就是把存在于一个库中的多个表，按照一定的纬度拆分到多个库中。这种拆分方式在微服务架构中也是很常见，基本上会按照业务纬度拆分数据库，同样该纬度也会影响到微服务的拆分，基本上服务和数据库是独立的。
      - 多库垂直拆分最大的好处就是实现了业务数据的隔离。其次就是缓解了请求的压力，原本所有的表在一个库的时候，所有请求都会打到一个数据库服务器上，通过数据库的拆分，可以分摊掉请求，在这个层面上提升了数据库的吞吐能力。
      
- 水平拆分
  - 单库水平分表
    - 如把一张有10000条数据的用户表，按照某种规则拆分成了4张表，每张表的数据量是2500条。
    - 但是注意，跟分区一样，这种方式虽然可以一定程度解决单表查询性能的问题，但是并不能解决单机存储瓶颈的问题。
  - 多库水平分表
    - 多库水平分表，其实有点类似于分库分表的综合实现方案，从分表来说是减少了单表的数据量，从分库层面来说，降低了单个数据库访问的性能瓶颈。

### 常见的水平分表策略
- 哈希取模分片 
  - 哈希分片，其实就是通过表中的某一个字段进行hash算法得到一个哈希值，然后通过取模运算确定数据应该放在哪个分片中。这种方式非常适合随机读写的场景中，它能够很好的将一个大表的数据随机分散到多个小表。
  - hash取模运算有个比较严重的问题，假设根据当前数据表的量以及增长情况，我们把一个大表拆分成了4个小表，看起来满足目前的需求，但是经过一段时间的运行后，发现四个表不够，需要再增加4个表来存储，这种情况下，就需要对原来的数据进行整体迁移，这个过程非常麻烦。 一般为了减少这种方式带来的数据迁移的影响，我们会采用一致性hash算法。
- 按照范围分片
  - 时间范围，比如我们按照数据创建时间，按照每一个月保存一个表。基于时间划分还可以用来做冷热数据分离，越早的数据访问频次越少。
  - 区域范围，区域一般指的是地理位置，比如一个表里面存储了来自全国各地的数据，如果数据量较大的情况下，可以按照地域来划分多个表。
  - 数据范围，比如根据某个字段的数据区间来进行划分。

### 数据库分区示例
查询当前数据库是否支持分区：show variables like '%partition%';
-- 查看是否为分区别：show table status where name = 'users';
-- 查看表的分区信息：show create table users;
-- 查看表具体的分区信息：select * from information_schema.PARTITIONS where TABLE_NAME = 'users'
-- 执行计划查询扫描了哪些分区：explain partitions select * from users where work_time < '2020-02-01';

MySQL支持的分区类型有 
- RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区。
- LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择。
- HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL 中有效的、产生非负整数值的任何表达式。
- KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值。

mysql中存储engine=innodb和engine=myisam的区别：
- innodb支持事务，支持行锁，数据和索引存放在表空间里面，支持分区
- myisam强调性能，支持再导出时跨平台迁移，表存放在单独的文件，也支持分区

range分区示例：按年份范围分区,MAXVALUE必须指定，否则超过最大值找不到分区将会报错
```
CREATE TABLE users (
id INT AUTO_INCREMENT,
amount DOUBLE NOT NULL,
order_day DATETIME NOT NULL,
PRIMARY KEY(id, order_day)
) ENGINE=Innodb
PARTITION BY RANGE(YEAR(order_day)) (
PARTITION p0 VALUES LESS THAN (2010),
PARTITION p1 VALUES LESS THAN (2011),
PARTITION p3 VALUES LESS THAN (2012),
PARTITION p_catchall VALUES LESS THAN MAXVALUE);
```
新增分区：
删除分区：ALERT TABLE users DROP PARTITION p0;  
合并分区（将 p0,p1 分区合并起来，放到新的 p0 分区中）： ALTER TABLE users REORGANIZE PARTITION p0,p1 INTO (PARTITION p0 VALUES LESS THAN (6000000));
用 REORGANIZE 方式重建分区的数量变成2，在这里数量只能减少不能增加。想要增加可以用 ADD PARTITION 方法。：ALTER TABLE users REORGANIZE PARTITION COALESCE PARTITION 2;  


### 数据库分表全局ID问题
以用户uid作为分表字段进行水平分表：user_info、user_info_01、user_info_02、user_info_03

如何实现全局唯一ID
- 数据库自增ID（定义全局表）
- UUID
- Redis的原子递增
- Twitter-Snowflake算法
- 美团的leaf
- MongoDB的ObjectId
- 百度的UidGenerator

分布式ID的特性
- 唯一性：确保生成的ID是全局唯一的。
- 有序递增性：确保生成的ID是对于某个用户或者业务是按一定的数字有序递增的。
- 高可用性：确保任何时候都能正确的生成ID。
- 带时间：ID里面包含时间，一眼扫过去就知道哪天的数据。

### 数据库ID自增方案
在数据库中专门创建一张序列表，利用数据库表中的自增ID来为其他业务的数据生成一个全局ID，那么每次要用ID的时候，直接从这个表中获取即可。
```
CREATE TABLE `uid_table` (
`id` bigint(20) NOT NULL AUTO_INCREMENT,
`business_id` int(11)  NOT NULL,
PRIMARY KEY (`id`) USING BTREE,
UNIQUE (business_type)
)
```
在应用程序中，每次调用下面这段代码，就可以持续获得一个递增的ID。
```
begin;
REPLACE INTO uid_table (business_id) VALUES (2);
SELECT LAST_INSERT_ID();
commit;
```
其中，replace into是每次删除原来相同的数据，同时加1条，就能保证我们每次得到的就是一个自增的ID。

> 这个方案的优点是非常简单，它也有缺点，就是对于数据库的压力比较大，而且最好是独立部署一个DB，而独立部署又会增加整体的成本。

优点：
- 非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护。
- ID号单调自增，可以实现一些对ID有特殊要求的业务。

缺点:
- 强依赖DB，当DB异常时整个系统不可用，属于致命问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。
- ID发号性能瓶颈限制在单台MySQL的读写性能。

#### UUID
UUID的格式是： xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx 8-4-4-4-12共36个字符，它是一个128bit的二进制转化为16进制的32个字符，然后用4个 - 连接起来的字符串。

UUID的五种生成方式:

- 基于时间的UUID（date-time & MAC address）： 主要依赖当前的时间戳及机器mac地址，因此可以保证全球唯一性。（使用了Mac地址，因此会暴露Mac地址和生成时间。）
- 分布式安全的UUID（date-time & group/user id）将版本1的时间戳前四位换为POSIX的UID或GID。
- 基于名字空间的UUID-MD5版（MD5 hash & namespace），基于指定的名字空间/名字生成MD5散列值得到，标准不推荐。
- 基于随机数的UUID（pseudo-random number）：基于随机数或伪随机数生成。
- 基于名字空间的UUID-SHA1版（SHA-1 hash & namespace）：将版本3的散列算法改为SHA1。
- 在Java中，提供了基于MD5算法的UUID、以及基于随机数的UUID。

优点:
- 本地生成，没有网络消耗，生成简单，没有高可用风险。

缺点:
- 不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。
- 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
- 无序查询效率低：由于生成的UUID是无序不可读的字符串，所以其查询效率低。
- UUID不适合用来做数据库的唯一ID，如果用UUID做主键，无序的不递增，大家都知道，主键是有 索引的，然后mysql的索引是通过b+树来实现的，每一次新的UUID数据的插入，为了查询的优 化，都会对索引底层的b+树进行修改，因为UUID数据是无序的，所以每一次UUID数据的插入都会对主键的b+树进行很大的修改，严重影响性能。

#### 雪花算法
SnowFlake 算法，是 Twitter 开源的分布式 id 生成算法。其核心思想就是：使用一个 64 bit 的 long 型的数字作为全局唯一 id。雪花算法比较常见，在百度的UidGenerator、美团的Leaf中，都有用到雪花算法的实现。

雪花算法的组成，一共64bit，这64个bit位由四个部分组成。

- 第一部分， 1bit位，用来表示符号位，而ID一般是正数，所以这个符号位一般情况下是0。
- 第二部分， 占41 个 bit：表示的是时间戳，是系统时间的毫秒数，但是这个时间戳不是当前系统的时间，而是当前 系统时间-开始时间 ，更大的保证这个ID生成方案的使用的时间！
  - 那么我们为什么需要这个时间戳，目的是为了保证有序性，可读性,我一看我就能猜到ID是什么时候生成的。
  > 41位可以2 41 - 1表示个数字，
  > 如果只用来表示正整数（计算机中正数包含0），可以表示的数值范围是：0 至 2 41 -1，减1
  > 是因为可表示的数值范围是从0开始算的，而不是1。
  > 也就是说41位可以表示2 41 -1个毫秒的值，转化成单位年则是(2 41 -1)/1000 * 60 * 60 * 24 * 365=69年，也就是能容纳69年的时间

- 第三部分， 用来记录工作机器id，id包含10bit，意味着这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。
  - 其中这10bit又可以分成2个5bit，前5bit表示机房id、5bit表示机器id，意味着最多支持2^5个机房（32），每个机房可以支持32台机器。
- 第四部分， 第四部分由12bit组成，它表示一个递增序列，用来记录同毫秒内产生的不同id。
  - 为什么需要这个序列号，设想下，如果是同一毫秒同一台机器来请求，那么我们怎么保证他的唯一性，这个时候，我们就能用到我们的序列号，
  - 目的是为了保证同一毫秒内同一机器生成的ID是唯一的，这个其实就是为了满足我们ID的这个高 并发，就是保证我同一毫秒进来的并发场景的唯一性。
  > 12位（bit）可以表示的最大正整数是2^12-1=4095，即可以用0、1、2、3、…4094这4095个数字，来表示同一机器同一时间截（毫秒)内产生的4095个ID序号。

雪花算法的优点：
1. 系统环境ID不重复：能满足高并发分布式系统环境ID不重复，比如大家熟知的分布式场景下的数据库表的ID生成。
2. 生成效率极高：在高并发，以及分布式环境下，除了生成不重复 id，每秒可生成百万个不重复 id，生成效率极高。
3. 保证基本有序递增：基于时间戳，可以保证基本有序递增，很多业务场景都有这个需求。
4. 不依赖第三方库：不依赖第三方的库，或者中间件，算法简单，在内存中进行。

缺点： 依赖服务器时间，服务器时钟回拨时可能会生成重复 id。

#### 非分片键查询
我们对user_info表的分片，是基于biz_id来实现的，也就是意味着如果我们想查询某张表的数据，必须先要使用biz_id路由找到对应的表才能查询到。

那么问题来了，如果查询的字段不是分片键（也就是不是biz_id），比如本次分库分表实战案例中，运营端查询就有根据名字、手机号、性别等字段来查，这时候我们并不知道去哪张表查询这些信息。

- 非分片键和分片键建立映射关系
第一种解决办法就是，把非分片键和分片键建立映射关系，比如login_name -> biz_id 建立映射，相当于建立一个简单的索引，当基于login_name查询数据时，先通过映射表查询出login_name对应的biz_id，再通过biz_id定位到目标表。

映射表的只有两列，可以承载很多的数据，当数据量过大时，也可以对映射表做水平拆分。 同时这种映射关系其实就是k-v键值对的关系，所以我们可以使用k-v缓存来存储提升性能。

同时因为这种映射关系的变更频率很低，所以缓存命中率很高，性能也很好。


### 开源分布式数据库中间件
- mycat
- ShardingSphere（包括 Sharding-JDBC、Sharding-Proxy 和 Sharding-Sidecar 3 款产品）。

两者异同点：
1）mycat是一个中间件的第三方应用，sharding-jdbc是一个jar包
2）使用mycat时不需要改代码，而使用sharding-jdbc时需要修改代码
3) mycat是db proxy, sharding-jdbc是jdbc proxy,
4) sharding-jdbc作为一个组件集成在应用内，而mycat则作为一个独立的应用需要单独部署，drds则是阿里云的一个独立产品，不过需要结合rds一起使用

### mycat
官网：http://www.mycat.org.cn/
github：https://github.com/MyCATApache/Mycat-Server
文档：http://www.mycat.org.cn/document/mycat-definitive-guide.pdf

Mycat 是基于阿里 Cobar 演变而来的一款开源分布式数据库中间件，是一个实现了 MySQL 协议的 Server。前端用户可以把它看做是一个数据库代理，用 MySQL 客户端工具和命令行访问；而其后端可以用 MySQL 原生（Native）协议与多个 MySQL 服务器通信，也可以用 JDBC 协议与大多数主流数据库服务器通信。

对于 DBA，MyCat 就是 MySQL Server，而 MyCat 后面连接的 MySQL Server 就好像是 MySQL 的存储引擎，如 InnoDB、MyISAM 等，因此 Mycat 本身并不存储数据，数据是在后端的 MySQL 上存储的，数据可靠性以及事务等都是由 MySQL 保证的。

对于软件工程师，MyCat 是一个近似等于 MySQL 的数据库服务器。你可以用连接 MySQL 的方式去连接 MyCat（除了端口不同，MyCat 默认端口是 8066 而非 3306），大多数情况下可以用你熟悉的对象映射框架使用 MyCat。但建议对于分片表，尽量使用基础的 SQL 语句，因为这样能达到最佳性能，特别是几千万甚至几百亿条记录的情况下。

对于架构师，MyCat 是一个强大的数据库中间件；不仅仅可以用作读写分离、以及分表分库、容灾备份，而且可以用于多租户应用开发、云平台基础设施。让你的架构具备很强的适应性和灵活性，借助于即将发布的 MyCat 智能优化模块，系统的数据访问瓶颈和热点一目了然，根据这些统计分析数据，你可以自动或手工调整后端存储，将不同的表映射到不同存储引擎上，而整个应用的代码一行也不用改变。

Mycat 2.0。提供的关键特性包括：

支持 SQL92 标准；
支持MySQL、Oracle、DB2、SQL Server、PostgreSQL 等 DB 的常见 SQL 语法；
遵守 MySQL 原生协议，跨语言，跨平台，跨数据库的通用中间件代理；
基于心跳的自动故障切换，支持读写分离，支持 MySQL 主从，以及 Galera Cluster 集群；
支持 Galera for MySQL 集群，Percona Cluster 或者 MariaDB cluster；
基于 Nio 实现，有效管理线程，解决高并发问题；
支持数据的多片自动路由与聚合，支持 sum、count、max 等常用的聚合函数，支持跨库分页；
支持单库内部任意 join，支持跨库 2表 join，甚至基于 caltlet 的多表 join；
支持通过全局表，ER 关系的分片策略，实现了高效的多表 join 查询；
支持多租户方案；
支持分布式事务（弱 xa）；
支持 XA 分布式事务（1.6.5）；
支持全局序列号，解决分布式下的主键生成问题；
分片规则丰富，插件化开发，易于扩展；
强大的 web，命令行监控；
支持前端作为 MySQL 通用代理，后端 JDBC 方式支持 Oracle、DB2、SQL Server 、 MongoDB 、巨杉；
支持密码加密；
支持服务降级；
支持 IP 白名单；
支持 SQL 黑名单、SQL 注入攻击拦截；
支持 prepare 预编译指令（1.6）；
支持非堆内存（Direct Memory）聚合计算（1.6）；
支持 PostgreSQL 的 native 协议（1.6）；
支持 mysql 和 Oracle 存储过程，out 参数、多结果集返回（1.6）；
支持 zookeeper 协调主从切换、zk 序列、配置 zk 化（1.6）；
支持库内分表（1.6）；
集群基于 ZooKeeper 管理，在线升级，扩容，智能优化，大数据处理（2.0开发版）。


### ShardingSphere
Apache ShardingSphere 是一套开源的分布式数据库中间件解决方案组成的生态圈，它由 Sharding-JDBC、Sharding-Proxy 和 Sharding-Sidecar（规划中）这 3 款相互独立，却又能够混合部署配合使用的产品组成。它们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如 Java 同构、异构语言、云原生等各种多样化的应用场景。

ShardingSphere 定位为关系型数据库中间件，旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。

ShardingSphere 项目由当当捐入 Apache，并在京东数科逐渐发展壮大，成为业界首个 Apache 分布式数据库中间件项目（据说当初 Mycat 立志也是希望加入 Apache）。ShardingSphere 已经在 2020 年 4 月 16 日成为 Apache 顶级项目（Apache官方发布从 4.0.0 版本开始）。

Sharding-JDBC
Sharding-JDBC 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。

- 适用于任何基于 JDBC 的 ORM 框架，如：JPA、Hibernate、Mybatis、Spring JDBC Template 或直接使用 JDBC。
- 支持任何第三方的数据库连接池，如：DBCP、C3P0、BoneCP、Druid、HikariCP 等。
- 支持任意实现 JDBC 规范的数据库。目前支持 MySQL、Oracle、SQLServer、PostgreSQL 以及任何遵循 SQL92 标准的数据库。

Sharding-Proxy
Sharding-Proxy 属于和 Mycat 对标的产品，它定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。目前先提供 MySQL/PostgreSQL 版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端（如：MySQL Command Client、MySQL Workbench、Navicat 等）操作数据，对 DBA 更加友好。

- 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用。
- 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端。

Sharding-Sidecar
Sharding-Sidecar 目前正在规划中，定位为 Kubernetes 的云原生数据库代理，以 Sidecar 的形式代理所有对数据库的访问。 通过无中心、零侵入的方案提供与数据库交互的的啮合层，即 Database Mesh，又可称数据网格。

Database Mesh 的关注重点在于如何将分布式的数据访问应用与数据库有机串联起来，它更加关注的是交互，是将杂乱无章的应用与数据库之间的交互有效的梳理。使用 Database Mesh，访问数据库的应用和数据库终将形成一个巨大的网格体系，应用和数据库只需在网格体系中对号入座即可，它们都是被啮合层所治理的对象。

混合架构
Sharding-JDBC 采用无中心化架构，适用于 Java 开发的高性能的轻量级 OLTP 应用；Sharding-Proxy 提供静态入口以及异构语言的支持，适用于 OLAP 应用以及对分片数据库进行管理和运维的场景。

ShardingSphere 是多接入端共同组成的生态圈。 通过混合使用 Sharding-JDBC 和 Sharding-Proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，架构师可以更加自由的调整适合于当前业务的最佳系统架构。


---

## 一致性hash算法

### 1.hash算法
哈希算法将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。哈希值是一段数据唯一且极其紧凑的数值表示形式。

普通的hash算法在分布式应用中的不足：

比如，在分布式的存储系统中，要将数据存储到具体的节点上，如果我们采用普通的hash算法进行路由，将数据映射到具体的节点上，如key%N，key是数据的key，N是机器节点数，如果有一个机器加入或退出这个集群，则所有的数据映射都无效了，如果是持久化存储则要做数据迁移，如果是分布式缓存，则其他缓存就失效了。

接下来我们来了解，一致性hash算法是怎么解决这个问题的。

### 2.一致性hash算法
一致性哈希提出了在动态变化的Cache环境中，哈希算法应该满足的4个适应条件(from 百度百科)：

- 均衡性(Balance)
  平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。

- 单调性(Monotonicity)
  单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。（这段翻译信息有负面价值的，当缓冲区大小变化时一致性哈希(Consistent hashing)尽量保护已分配的内容不会被重新映射到新缓冲区。）

- 分散性(Spread)
  在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。

- 负载(Load)
  负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

接下来说一下具体的设计：

#### 2.1环形hash空间
按照常用的hash算法来将对应的key哈希到一个具有2^32次方个节点的空间中，即0 ~ (2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。

NOTE:当然，节点的个数可以自定义，整个hash环我们可以用TreeMap来实现，因为treeMap是排序的，我们刚好可以利用上。

#### 2.2映射服务器节点
将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或唯一主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假设我们将四台服务器使用ip地址哈希后在环空间的位置如下：

#### 2.3映射数据
现在我们将objectA、objectB、objectC、objectD四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上,然后从数据所在位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。

#### 2.4服务器的删除与添加
- 2.4.1如果此时NodeC宕机了，此时Object A、B、D不会受到影响，只有Object C会重新分配到Node D上面去，而其他数据对象不会发生变化

- 2.4.2如果在环境中新增一台服务器Node X，通过hash算法将Node X映射到环中，通过按顺时针迁移的规则，那么Object C被迁移到了Node X中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。

#### 2.5.虚拟节点
到目前为止一致性hash也可以算做完成了，但是有一个问题还需要解决，那就是平衡性。从下图我们可以看出，当服务器节点比较少的时候，会出现一个问题，就是此时必然造成大量数据集中到一个节点上面，极少数数据集中到另外的节点上面。

为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以先确定每个物理节点关联的虚拟节点数量，然后在ip或者主机名后面增加编号。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：

同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。每个物理节点关联的虚拟节点数量就根据具体的生产环境情况在确定。


## MySQL数据库迁移+动态扩容缩容的分库分表

> 如何设计可以动态扩容缩容的分库分表方案？

- 停机扩容(不推荐）

这个方案就跟停机迁移一样,步骤几乎一致,唯一的一点就是那个导数的工具,是把现有库表的数据抽出来慢慢倒入到新的库和表里去。

缺点：风险高，数据量过大时，耗时过长，无法保证服务稳定性，

- 不停机扩容(在线双写)
  
在不停机条件下需要对数据的迁移或者扩容,这里推荐我们常用的一种方案,也就是在线双写的机制。

1. 通过在写原有的数据库的同时也写一份数据到我们的新的库表中。
2. 同样写一个后台迁移数据的程序,将我们的旧库的数据通过我们的数据库中间件迁移到新的多库表中。
3. 在迁移的过程中,每次插入数据的时候,还需要检测数据的更新情况。比如,如果新的表中没有当前的数据,则直接新增；如果新表有数据并没有我们要迁移的数据新的话,我们就更新为当前数据,只能允许新的数据覆盖旧的数据(这里其实推荐使用Canal这样的数据库同步中间件对增量更新做同步）。
4. 经过一轮之后,也就是假如旧表中1000万条旧数据迁移完之后,我们就需要进行校验,校验两边数据是否是一模一样的。
5. 这样反复的跑了几天之后,就数据库和新的数据库肯定是会一模一样的,最后观察下数据正常了，就可以停掉旧库的写入动作了。


### 优化后的方案
一开始上来就是 32 个库,每个库 32 个表,那么总共是 1024 张表。基本上国内的互联网肯定都是够用了,无论是并发支撑还是数据量支撑都没问题。

每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载 32 * 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。

有些除非是国内排名非常靠前的这些公司,他们的最核心的系统的数据库,可能会出现几百台数据库的这么一个规模,128 个库,256 个库,512 个库。

1024 张表,假设每个表放 500 万数据,在 MySQL 里可以放 50 亿条数据。

每秒5万的写并发,总共 50 亿条数据,对于国内大部分的互联网公司来说,其实一般来说都够了。

分库分表路由规则：根据某个 id 先根据 32 取模路由到库,再根据 32 取模路由到库里的表。
```
|orderId	|id % 32 (库)	|id / 32 % 32 (表)|
|---|---|---|
|259	|3	    |8|
|1189	|5	    |5|
|352	|0	    |11|
|4593	|17     |15|
```
刚开始的时候,这个库可能就是逻辑库,建在一个数据库上的,就是一个,mysql服务器可能建了n个库比如 32 个库。后面如果要拆分,就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。

比如说最多可以扩展到 32 个数据库服务器,每个数据库服务器是一个库。如果还是不够？最多可以扩展到1024个数据库服务器,每个数据库服务器上面一个库一个表。

这么搞,是不用自己写代码做数据迁移的,都交给 dba 来搞好了,但是 dba 确实是需要做一些库表迁移的工作,但是总比你自己写代码,然后抽数据导数据来的效率高得多吧。

哪怕是要减少库的数量,也很简单,其实说白了就是按倍数缩容就可以了,然后修改一下路由规则。

> 这里对步骤做一个总结：

1. 设定好几台数据库服务器,每台服务器上几个库,每个库多少个表,推荐是 32 库 * 32 表,对于大部分公司来说,可能几年都够了。
2. 路由的规则,orderId 模 32 = 库，orderId / 32 模 32 = 表
3. 扩容的时候,申请增加更多的数据库服务器,装好 mysql,呈倍数扩容,4 台服务器,扩到8台服务器,再到16台服务器。
4. 由dba负责将原先数据库服务器的库,迁移到新的数据库服务器上去,库迁移是有一些便捷的工具的。
5. 我们这边就是修改一下配置,调整迁移的库所在数据库服务器的地址。
6. 重新发布系统,上线,原先的路由规则变都不用变,直接可以基于 n 倍的数据库服务器的资源,继续进行线上系统的提供服务。


 





